{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sarjanagpt22/GarudaAI_2030/blob/main/Multilayer_Perceptron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nJTbCftpNaT"
      },
      "source": [
        "MULTILAYER PURE NUMPY\n",
        "(konsep dasar)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Secara konsep matematis, kode ini menggambarkan proses sederhana dari jaringan saraf tiruan (neural network) dengan dua lapisan: satu hidden layer dan satu output layer. Persamaan dasarnya mengikuti rumus , di mana  adalah input,  adalah bobot (weight), dan  adalah bias. Hasil  kemudian dilewatkan ke fungsi aktivasi ReLU (Rectified Linear Unit), yang secara matematis ditulis sebagai . ReLU membuat nilai negatif menjadi nol dan mempertahankan nilai positifnya, sehingga menambah sifat non-linear pada model—bagian penting dari jaringan saraf agar bisa mempelajari hubungan kompleks.\n",
        "\n",
        "Dari sisi logika pemrograman, kode ini menggunakan numpy untuk menghitung operasi vektor dan matriks dengan efisien. Pertama, program menghasilkan input acak 3x3 (x = np.random.randn(3, 3)), lalu mengalikannya dengan matriks bobot w1 berukuran sama menggunakan np.dot(x, w1) untuk menghasilkan z1, kemudian menambahkan bias b1 (yang diinisialisasi dengan nol). Nilai z1 kemudian dilewatkan ke fungsi relu() agar elemen negatif diubah menjadi nol. Langkah ini disimpan sebagai a1, yang berfungsi sebagai output dari hidden layer sekaligus input ke output layer.\n",
        "\n",
        "Di bagian output layer, hasil a1 kembali dikalikan dengan bobot kedua w2 menggunakan operasi dot product yang sama. Nilainya disimpan di z2, yang merepresentasikan output akhir dari jaringan. Karena tidak ada fungsi aktivasi tambahan di lapisan ini, hasil akhirnya hanyalah kombinasi linear dari nilai-nilai ReLU sebelumnya. Loop for sebanyak 20 kali membuat proses ini diulang untuk 20 input acak berbeda, lalu mencetak hasil matriks output untuk tiap iterasi. Jadi, inti logika program ini adalah mensimulasikan aliran data dari input → bobot → fungsi aktivasi → output dengan struktur jaringan 3-neuron per layer."
      ],
      "metadata": {
        "id": "ceyxayzvWEXh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBhJFib-okj3",
        "outputId": "0db106d7-e851-448e-898c-be4f363e1c03",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ouput = [[0.         0.         0.        ]\n",
            " [0.00820416 0.04102079 0.06235161]\n",
            " [0.01119761 0.05598804 0.08510182]]\n",
            "Ouput = [[0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [1.33155304e-03 6.65776518e-03 1.01198031e-02]\n",
            " [1.49480393e+00 2.94561344e+00 5.84592668e+00]]\n",
            "Ouput = [[0.10494663 0.31449575 0.55816185]\n",
            " [0.         0.         0.        ]\n",
            " [0.03694614 0.12758176 0.22311257]]\n",
            "Ouput = [[0.66404099 1.08483691 2.36293465]\n",
            " [0.28398798 0.79796795 1.4920491 ]\n",
            " [0.41306175 0.78523658 1.579394  ]]\n",
            "Ouput = [[0.06882493 0.34412463 0.52306944]\n",
            " [0.26037505 0.36206049 0.81917371]\n",
            " [0.         0.         0.        ]]\n",
            "Ouput = [[0.00214781 0.00384815 0.00456409]\n",
            " [0.         0.         0.        ]\n",
            " [0.21871606 0.86274266 1.4249378 ]]\n",
            "Ouput = [[0.35905719 0.61800023 1.26157342]\n",
            " [0.05442959 0.06295461 0.1503316 ]\n",
            " [0.22695965 0.33165611 0.71886007]]\n",
            "Ouput = [[0.03206434 0.0574486  0.06813672]\n",
            " [0.         0.         0.        ]\n",
            " [0.03330411 0.0615268  0.10287578]]\n",
            "Ouput = [[0.70718696 1.16409438 2.44921248]\n",
            " [0.5736858  1.15914344 2.1976244 ]\n",
            " [0.24793758 0.27377732 0.6978226 ]]\n",
            "Ouput = [[0.         0.         0.        ]\n",
            " [0.01306324 0.06317566 0.09562781]\n",
            " [0.         0.         0.        ]]\n",
            "Ouput = [[0.09776715 0.11540094 0.26769962]\n",
            " [0.50174839 1.19822071 2.32518132]\n",
            " [0.02561177 0.04588775 0.05442501]]\n",
            "Ouput = [[0.         0.         0.        ]\n",
            " [0.         0.         0.        ]\n",
            " [0.00514867 0.0092247  0.01094093]]\n",
            "Ouput = [[0.         0.         0.        ]\n",
            " [0.01721715 0.08608575 0.13085034]\n",
            " [0.04515773 0.22578864 0.34319874]]\n",
            "Ouput = [[0.         0.         0.        ]\n",
            " [0.5213259  0.69770104 1.67808817]\n",
            " [0.47062967 0.88588966 1.80137615]]\n",
            "Ouput = [[0.         0.         0.        ]\n",
            " [0.         0.         0.        ]\n",
            " [0.2019623  0.29505084 0.58851918]]\n",
            "Ouput = [[0.31108157 0.68462252 1.46584377]\n",
            " [0.5863092  0.93756778 2.04957401]\n",
            " [0.         0.         0.        ]]\n",
            "Ouput = [[0.42865512 0.75170515 1.45129276]\n",
            " [0.         0.         0.        ]\n",
            " [0.         0.         0.        ]]\n",
            "Ouput = [[0.02101453 0.01428988 0.06808708]\n",
            " [0.26277743 0.35524218 0.89396984]\n",
            " [0.         0.         0.        ]]\n",
            "Ouput = [[0.         0.         0.        ]\n",
            " [0.21587923 0.35339798 0.79255598]\n",
            " [0.         0.         0.        ]]\n",
            "Ouput = [[0.         0.         0.        ]\n",
            " [0.25736614 0.35614019 0.65219197]\n",
            " [0.         0.         0.        ]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "#Fungsi Aktivasi ReLU\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "#Input Dot Product 3x3\n",
        "\n",
        "#Input Layer\n",
        "np.random.seed(50)\n",
        "for i in range(20):\n",
        "    x = np.random.randn(3, 3)\n",
        "    w1 = np.array([[0.31, 0.69, 0.57], [0.25, 0.39, 0.70], [0.44, 0.55, 0.15]])\n",
        "    b1 = np.zeros((3, 3))\n",
        "    z1 = np.dot(x, w1) + b1\n",
        "    a1 = relu(z1)\n",
        "\n",
        "#Output Layer\n",
        "    w2 = np.array([[0.24, 0.43, 0.51], [0.25, 0.17, 0.81], [0.10, 0.50, 0.76]])\n",
        "    b2 = np.zeros((3, 3))\n",
        "    z2 = np.dot(a1, w2) + b2\n",
        "    print(\"Ouput =\", z2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ini adalah list Tuple menghasilkan angka acak dari matriks 3×3 , berguna untuk inisialisasi bobot acak pada Neural Network. Dan juga list Tuple Matriks nol 3×3 untuk inisialisasi bias."
      ],
      "metadata": {
        "id": "hd9_NGJlZIES"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGpL2uLVyVJl",
        "outputId": "b9f0d26c-3af7-412b-b471-eb6033082bfb",
        "collapsed": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[ 0.42680606 -0.53088771  0.2478494 ]\n",
            " [ 0.34585822 -1.06123493 -1.28951216]\n",
            " [ 0.88393228 -0.83821332  0.23066455]]\n",
            "[[0. 0. 0.]\n",
            " [0. 0. 0.]\n",
            " [0. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "#list tuple menghasilkan angka random\n",
        "x = np.random.randn(3, 3)\n",
        "\n",
        "#list tuple nilai 0 3x3\n",
        "b = np.zeros((3, 3))\n",
        "print(x)\n",
        "print(b)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ini implementasi manual gradient descent untuk regresi linear sederhana. Secara matematis model yang dipelajari adalah. Fungsi loss yang dipakai adalah Mean Squared Error (MSE) Turunan parsial terhadap parameter diturunkan dari MSE dan Gradien itu memberi arah penurunan loss ambil langkah kecil ke arah negatif gradien untuk memperbaiki  dan .\n",
        "\n",
        "Dari sisi logika pemrograman, kode memanfaatkan operasi vektor numpy agar komputasi efisien: y_pred = w * x + b menghasilkan prediksi untuk seluruh dataset sekaligus, error = y - y_pred vektornya, dan loss = np.mean(error**2) menghitung MSE. Gradien dihitung juga secara vektorial (np.sum(x * error) dan np.sum(error)), lalu parameter diperbarui setiap epoch lewat w = w - lr * grad_w dan b = b - lr * grad_b. Loop for epoch in range(epochs) melakukan training selama jumlah epoch yang ditentukan; blok if epoch % 400 == 0 cuma buat logging berkala supaya nggak banjir output.\n",
        "\n",
        "Praktisnya dengan data seharusnya model konvergen ke dan kalau learning rate dan epoch cukup, karena datasetnya linear sempurna. Perhatian: lr terlalu besar bisa bikin osilasi atau divergen, terlalu kecil bikin lambat, inisialisasi nol di sini fine untuk linear sederhana, tapi untuk model lain random small biasanya lebih aman. Ini full-batch gradient descent (pakai semua data tiap update) alternatif lebih cepat untuk dataset besar: mini-batch atau optimizer adaptif, atau kalau cuma linear regression hitung solusi analitik langsung dengan normal equation untuk hasil exact."
      ],
      "metadata": {
        "id": "YH-uM9UGZ-JD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yl-3911oG_d7",
        "outputId": "077ee749-4b03-47bc-cb0d-70fbd84bf83b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch    0 | loss 41.000000 | w 0.3500 b 0.1200\n",
            "epoch  400 | loss 0.001246 | w 2.0293 b 0.9139\n",
            "epoch  800 | loss 0.000113 | w 2.0088 b 0.9740\n",
            "epoch 1200 | loss 0.000010 | w 2.0027 b 0.9922\n",
            "epoch 1600 | loss 0.000001 | w 2.0008 b 0.9976\n",
            "hasil akhir: w= 2.0002424048651353 b= 0.9992873001360325\n"
          ]
        }
      ],
      "source": [
        "# Manual gradient descent untuk linear regression\n",
        "import numpy as np\n",
        "\n",
        "# data contoh\n",
        "x = np.array([1.0, 2.0, 3.0, 4.0])\n",
        "y = np.array([3.0, 5.0, 7.0, 9.0])  # y = 2*x + 1\n",
        "\n",
        "# inisialisasi\n",
        "w = 0.0\n",
        "b = 0.0\n",
        "lr = 0.01\n",
        "n = len(x)\n",
        "epochs = 2000\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    y_pred = w * x + b\n",
        "    error = y - y_pred\n",
        "    loss = np.mean(error**2)  # MSE\n",
        "\n",
        "    # turunan parsial (gradien)\n",
        "    # ∂L/∂w = -2/n * sum(x * (y - (w*x + b)))\n",
        "    # ∂L/∂b = -2/n * sum(y - (w*x + b))\n",
        "    grad_w = -2.0/n * np.sum(x * error)\n",
        "    grad_b = -2.0/n * np.sum(error)\n",
        "\n",
        "    # update parameter (gradient descent)\n",
        "    w = w - lr * grad_w\n",
        "    b = b - lr * grad_b\n",
        "\n",
        "    if epoch % 400 == 0:\n",
        "        print(f\"epoch {epoch:4d} | loss {loss:.6f} | w {w:.4f} b {b:.4f}\")\n",
        "\n",
        "print(\"hasil akhir:\", \"w=\", w, \"b=\", b)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Secara konsep matematis, kode ini merepresentasikan proses feedforward neural network dua lapis yang dilatih menggunakan metode gradient descent. Fungsi modelnya adalah , di mana  adalah data input,  adalah parameter jaringan, dan fungsi aktivasi ReLU membuat model bersifat non-linear. Tujuan training adalah meminimalkan loss function Mean Squared Error (MSE): , dengan  sebagai target output. Penurunan nilai loss dilakukan lewat backpropagation, yaitu menghitung turunan parsial terhadap parameter untuk memperbarui bobot sesuai arah negatif gradien: , dengan  sebagai learning rate.\n",
        "\n",
        "Dari sisi logika pemrograman, kode memanfaatkan numpy untuk melakukan operasi matriks cepat seperti np.dot() (perkalian linear) dan broadcasting pada bias. Bagian awal mendefinisikan jaringan dengan bobot acak kecil, lalu membuat target data dari jaringan “teacher” (parameter acak lain) agar model punya arah pembelajaran yang jelas. Proses utama ada dalam loop training: bagian forward pass menghitung aktivasi layer demi layer, lalu loss diukur dengan np.mean(error**2). Setelah itu, backward pass menghitung gradien grad_w1, grad_b1, grad_w2, grad_b2 lewat aturan rantai (chain rule) dan fungsi turunan ReLU (relu_grad). Setiap epoch, parameter diperbarui berdasarkan gradien agar output model makin mendekati target.\n",
        "\n",
        "Konsepnya mirip proses belajar manusia: model menebak hasil, menghitung seberapa besar kesalahannya, lalu menyesuaikan diri sedikit demi sedikit di tiap iterasi. Dalam praktik, ini adalah bentuk paling dasar dari deep learning tanpa library tinggi seperti TensorFlow atau PyTorch. Loop training berulang selama 500 epoch supaya bobot mendekati nilai optimal. if epoch % 50 == 0 hanya mencetak hasil sesekali agar bisa diamati konvergensinya, dan saat loss menurun bertahap, itu berarti model berhasil “belajar” bukan hanya menghitung, tapi beradaptasi lewat perhitungan matematis yang konsisten."
      ],
      "metadata": {
        "id": "x9PAiHFcrQMp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "np.random.seed(42)\n",
        "input_data = np.random.randn(6, 4)\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_grad(x):\n",
        "    return (x > 0).astype(float)\n",
        "\n",
        "D = input_data.shape[1]\n",
        "H = 5\n",
        "\n",
        "w1 = np.random.randn(D, H) * 0.1\n",
        "b1 = np.zeros((1, H))\n",
        "w2 = np.random.randn(H, D) * 0.1\n",
        "b2 = np.zeros((1, D))\n",
        "\n",
        "z1 = input_data.dot(w1) + b1\n",
        "a1 = relu(z1)\n",
        "z2 = a1.dot(w2) + b2\n",
        "target = relu(z2)\n",
        "\n",
        "lr = 0.01\n",
        "epochs = 500\n",
        "N = input_data.shape[0]\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    z1 = input_data.dot(w1) + b1\n",
        "    a1 = relu(z1)\n",
        "    z2 = a1.dot(w2) + b2\n",
        "    y_pred = relu(z2)\n",
        "\n",
        "    error = y_pred - target\n",
        "    loss = np.mean(error**2)\n",
        "\n",
        "    dz2 = (2.0 / N) * error * relu_grad(z2)\n",
        "    grad_w2 = a1.T.dot(dz2)\n",
        "    grad_b2 = np.sum(dz2, axis=0, keepdims=True)\n",
        "\n",
        "    dz1 = dz2.dot(w2.T) * relu_grad(z1)\n",
        "    grad_w1 = input_data.T.dot(dz1)\n",
        "    grad_b1 = np.sum(dz1, axis=0, keepdims=True)\n",
        "\n",
        "    w2 -= lr * grad_w2\n",
        "    b2 -= lr * grad_b2\n",
        "    w1 -= lr * grad_w1\n",
        "    b1 -= lr * grad_b1\n",
        "\n",
        "    if epoch % 100 == 0 or epoch == 1:\n",
        "        print(f\"epoch {epoch:4d} | loss {loss:.10f}\")\n",
        "\n",
        "print(\"hasil akhir:\", \"loss=\", loss)\n",
        "print(\"w1=\", w1)\n",
        "print(\"b1=\", b1)\n",
        "print(\"w2=\", w2)\n",
        "print(\"b2=\", b2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlcHS-PFfkuf",
        "outputId": "93aa23b0-a00d-44c0-8d54-92c27ab1164e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch    1 | loss 0.0000000000\n",
            "epoch  100 | loss 0.0000000000\n",
            "epoch  200 | loss 0.0000000000\n",
            "epoch  300 | loss 0.0000000000\n",
            "epoch  400 | loss 0.0000000000\n",
            "epoch  500 | loss 0.0000000000\n",
            "hasil akhir: loss= 0.0\n",
            "w1= [[-0.05443827  0.01109226 -0.11509936  0.0375698  -0.06006387]\n",
            " [-0.02916937 -0.06017066  0.18522782 -0.00134972 -0.10577109]\n",
            " [ 0.08225449 -0.12208436  0.02088636 -0.19596701 -0.1328186 ]\n",
            " [ 0.01968612  0.07384666  0.01713683 -0.01156483 -0.03011037]]\n",
            "b1= [[0. 0. 0. 0. 0.]]\n",
            "w2= [[-0.1478522  -0.07198442 -0.04606388  0.10571222]\n",
            " [ 0.03436183 -0.17630402  0.0324084  -0.03850823]\n",
            " [-0.0676922   0.06116763  0.10309995  0.09312801]\n",
            " [-0.08392175 -0.03092124  0.03312634  0.09755451]\n",
            " [-0.04791742 -0.0185659  -0.1106335  -0.11962066]]\n",
            "b2= [[0. 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "np.random.seed(42)\n",
        "X = np.random.randn(6, 4)\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_grad(x):\n",
        "    return (x > 0).astype(float)\n",
        "\n",
        "D = X.shape[1]\n",
        "H = 5\n",
        "\n",
        "w1 = np.random.randn(D, H) * 0.1\n",
        "b1 = np.zeros((1, H))\n",
        "w2 = np.random.randn(H, D) * 0.1\n",
        "b2 = np.zeros((1, D))\n",
        "\n",
        "true_w1 = np.random.randn(D, H) * 0.1\n",
        "true_b1 = np.random.randn(1, H) * 0.1\n",
        "true_w2 = np.random.randn(H, D) * 0.1\n",
        "true_b2 = np.random.randn(1, D) * 0.1\n",
        "\n",
        "z1_t = X.dot(true_w1) + true_b1\n",
        "a1_t = relu(z1_t)\n",
        "z2_t = a1_t.dot(true_w2) + true_b2\n",
        "target = relu(z2_t) + 0.01 * np.random.randn(*z2_t.shape)\n",
        "\n",
        "lr = 0.01\n",
        "epochs = 500\n",
        "N = X.shape[0]\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    z1 = X.dot(w1) + b1\n",
        "    a1 = relu(z1)\n",
        "    z2 = a1.dot(w2) + b2\n",
        "    y_pred = relu(z2)\n",
        "\n",
        "    error = y_pred - target\n",
        "    loss = np.mean(error**2)\n",
        "\n",
        "    dz2 = (2.0 / N) * error * relu_grad(z2)\n",
        "    grad_w2 = a1.T.dot(dz2)\n",
        "    grad_b2 = np.sum(dz2, axis=0, keepdims=True)\n",
        "\n",
        "    dz1 = dz2.dot(w2.T) * relu_grad(z1)\n",
        "    grad_w1 = X.T.dot(dz1)\n",
        "    grad_b1 = np.sum(dz1, axis=0, keepdims=True)\n",
        "\n",
        "    w2 -= lr * grad_w2\n",
        "    b2 -= lr * grad_b2\n",
        "    w1 -= lr * grad_w1\n",
        "    b1 -= lr * grad_b1\n",
        "\n",
        "    if epoch % 50 == 0 or epoch == 1:\n",
        "        print(f\"epoch {epoch:4d} | loss {loss:.10f}\")\n",
        "\n",
        "print(\"hasil akhir:\", \"loss=\", loss)\n",
        "print(\"w1 shape\", w1.shape, \"w2 shape\", w2.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1FqbXcMhjRr",
        "outputId": "8dc93cdd-a66c-45a2-d77f-6eaa948eed85",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch    1 | loss 0.0003584421\n",
            "epoch   50 | loss 0.0003381986\n",
            "epoch  100 | loss 0.0003346269\n",
            "epoch  150 | loss 0.0003280211\n",
            "epoch  200 | loss 0.0003235128\n",
            "epoch  250 | loss 0.0003215919\n",
            "epoch  300 | loss 0.0003215232\n",
            "epoch  350 | loss 0.0003214549\n",
            "epoch  400 | loss 0.0003214203\n",
            "epoch  450 | loss 0.0003213677\n",
            "epoch  500 | loss 0.0003213252\n",
            "hasil akhir: loss= 0.0003213252047370849\n",
            "w1 shape (4, 5) w2 shape (5, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Secara matematis, kode ini merepresentasikan jaringan saraf tiruan sederhana dengan tiga lapisan linear dan fungsi aktivasi ReLU. Setiap lapisan menjalankan transformasi linier , lalu hasilnya dilewatkan ke fungsi aktivasi ReLU yang secara matematis didefinisikan sebagai . Proses ini dilakukan berurutan dari layer pertama hingga ketiga, menciptakan fungsi komposisi non-linear yang memetakan input acak menuju output prediksi . Perbedaan utama dengan model linear biasa adalah ReLU yang membuat model bisa meniru hubungan non-linear antara input dan target .\n",
        "\n",
        "Dari sisi optimisasi, training dilakukan dengan konsep Mean Squared Error (MSE) sebagai fungsi loss, yaitu . Turunan parsialnya terhadap bobot dan bias di layer terakhir dihitung menggunakan aturan rantai (chain rule) dan gradien dari ReLU. Gradien tersebut lalu digunakan dalam langkah pembaruan parameter  dan , di mana  adalah learning rate. Mekanisme ini memastikan bahwa setiap epoch menyesuaikan nilai  dan  agar prediksi jaringan makin mendekati target yang diinginkan.\n",
        "\n",
        "Secara logika pemrograman, struktur kodenya dibagi menjadi tiga tahap besar: inisialisasi, forward pass, dan training loop. Inisialisasi membuat input acak, bobot, dan bias. Lalu dalam setiap iterasi (epoch), program menghitung keluaran jaringan lewat operasi np.dot() dan fungsi ReLU. Setelah mendapatkan prediksi dan error, kode menghitung gradien serta memperbarui parameter. Loop training berlangsung ribuan kali agar model “belajar” dari perbedaan antara prediksi dan target. Secara logika, ini mirip sistem kontrol otomatis: model terus menyesuaikan dirinya dengan umpan balik error sampai sistem stabil di sekitar nilai loss terkecil."
      ],
      "metadata": {
        "id": "6L5RsgSZt9Sj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5xDk5HLuZFN",
        "outputId": "d4d15611-0209-447f-bbe9-656eb9e12c25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch    1 | loss 0.18532455\n",
            "epoch  200 | loss 0.16365499\n",
            "epoch  400 | loss 0.16364509\n",
            "epoch  600 | loss 0.16364218\n",
            "epoch  800 | loss 0.16363927\n",
            "epoch 1000 | loss 0.16363636\n",
            "epoch 1200 | loss 0.16363345\n",
            "epoch 1400 | loss 0.16363054\n",
            "epoch 1600 | loss 0.16362763\n",
            "epoch 1800 | loss 0.16362472\n",
            "epoch 2000 | loss 0.16362182\n",
            "hasil akhir loss = 0.1636218153247194\n",
            "a3 terakhir =\n",
            " [[2.02892924e-01 1.81445302e-01 0.00000000e+00 0.00000000e+00\n",
            "  1.93085282e-01]\n",
            " [1.98568022e-01 1.79985591e-01 0.00000000e+00 0.00000000e+00\n",
            "  1.92085797e-01]\n",
            " [1.99416137e-01 1.79532339e-01 0.00000000e+00 0.00000000e+00\n",
            "  1.91604129e-01]\n",
            " [1.99525383e-01 1.79575336e-01 0.00000000e+00 0.00000000e+00\n",
            "  1.91755284e-01]\n",
            " [1.99579249e-01 1.79445100e-01 3.31017420e-07 1.61718920e-08\n",
            "  1.91452131e-01]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_grad(x):\n",
        "    return (x > 0).astype(float)\n",
        "\n",
        "X = np.random.randn(5, 5)\n",
        "w1 = np.random.randn(5, 5) * 0.1\n",
        "b1 = np.zeros((1, 5))\n",
        "w2 = np.random.randn(5, 5) * 0.1\n",
        "b2 = np.zeros((1, 5))\n",
        "w3 = np.random.randn(5, 5) * 0.1\n",
        "b3 = np.zeros((1, 5))\n",
        "\n",
        "z1 = X.dot(w1) + b1\n",
        "a1 = relu(z1)\n",
        "z2 = a1.dot(w2) + b2\n",
        "a2 = relu(z2)\n",
        "z3 = a2.dot(w3) + b3\n",
        "a3 = relu(z3)\n",
        "\n",
        "x_true = np.array([\n",
        "    [0.0, 0.0, 0.0, 0.0, 0.0],\n",
        "    [0.0, 0.0, 0.0, 0.0, 0.0],\n",
        "    [1.0, 0.9, 0.95, 1.0, 0.96],\n",
        "    [0.0, 0.0, 0.0, 0.0, 0.0],\n",
        "    [0.0, 0.0, 0.0, 0.0, 0.0]\n",
        "])\n",
        "\n",
        "lr = 0.01\n",
        "epochs = 2000\n",
        "N = X.shape[0]\n",
        "\n",
        "for ep in range(1, epochs + 1):\n",
        "    z1 = X.dot(w1) + b1\n",
        "    a1 = relu(z1)\n",
        "    z2 = a1.dot(w2) + b2\n",
        "    a2 = relu(z2)\n",
        "    z3 = a2.dot(w3) + b3\n",
        "    y_pred = relu(z3)\n",
        "\n",
        "    error = y_pred - x_true\n",
        "    loss = np.mean(error**2)\n",
        "\n",
        "    dz3 = (2.0 / N) * error * relu_grad(z3)\n",
        "    grad_w3 = a2.T.dot(dz3)\n",
        "    grad_b3 = np.sum(dz3, axis=0, keepdims=True)\n",
        "\n",
        "    w3 -= lr * grad_w3\n",
        "    b3 -= lr * grad_b3\n",
        "\n",
        "    if ep % 200 == 0 or ep == 1:\n",
        "        print(f\"epoch {ep:4d} | loss {loss:.8f}\")\n",
        "\n",
        "print(\"hasil akhir loss =\", loss)\n",
        "print(\"a3 terakhir =\\n\", y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Secara matematis, kode ini merepresentasikan feedforward neural network dua lapis dengan satu hidden layer berukuran 4 neuron. Proses forward propagation mengikuti persamaan: , lalu diaktifkan menggunakan fungsi sigmoid , kemudian dilanjutkan ke layer output  dan diaktifkan lagi . Fungsi sigmoid sendiri secara matematis adalah , yang membatasi output di antara 0 dan 1. Model ini cocok untuk tugas klasifikasi biner seperti XOR, di mana hubungan antara input dan output bersifat non-linear.\n",
        "\n",
        "Pada tahap backpropagation, kode menghitung gradien turunan dari fungsi Mean Squared Error (MSE) terhadap setiap parameter: bobot dan bias. Dengan menggunakan chain rule, gradien layer output dihitung sebagai , lalu diturunkan kembali ke layer tersembunyi melalui , di mana  adalah turunan sigmoid. Setelah gradien dihitung, pembaruan bobot dilakukan dengan aturan gradient descent: , di mana  adalah learning rate. Setiap epoch, loss dihitung dan digunakan untuk mengukur seberapa dekat prediksi dengan target.\n",
        "\n",
        "Dari sisi logika pemrograman, struktur kodenya modular dan berorientasi objek. Kelas SimpleNeuralNetwork membungkus seluruh mekanisme pembelajaran ke dalam fungsi terpisah forward, backward, dan update_parameters. Program utama membuat dataset XOR sederhana, menginisialisasi jaringan, dan menjalankan training hingga 10.000 epoch. Tiap loop memanggil forward pass untuk menghasilkan prediksi, backward pass untuk menghitung gradien, lalu memperbarui parameter berdasarkan hasil tersebut. Hasil akhirnya menunjukkan bahwa model berhasil “belajar” dari data, dengan prediksi mendekati target XOR klasik (0, 1, 1, 0). Kode ini adalah versi mini dari cara kerja otak buatan, komputasi diferensial yang disusun dalam loop pembelajaran berulang."
      ],
      "metadata": {
        "id": "buKqNzXLvvW_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D1Araw-RiFYU",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86412869-b3c4-4af5-eef8-274830d7cfdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss 0.2500\n",
            "Epoch 100, Loss 0.2500\n",
            "Epoch 200, Loss 0.2500\n",
            "Epoch 300, Loss 0.2500\n",
            "Epoch 400, Loss 0.2500\n",
            "Epoch 500, Loss 0.2500\n",
            "Epoch 600, Loss 0.2500\n",
            "Epoch 700, Loss 0.2500\n",
            "Epoch 800, Loss 0.2500\n",
            "Epoch 900, Loss 0.2500\n",
            "Epoch 1000, Loss 0.2500\n",
            "Epoch 1100, Loss 0.2500\n",
            "Epoch 1200, Loss 0.2500\n",
            "Epoch 1300, Loss 0.2500\n",
            "Epoch 1400, Loss 0.2500\n",
            "Epoch 1500, Loss 0.2500\n",
            "Epoch 1600, Loss 0.2500\n",
            "Epoch 1700, Loss 0.2500\n",
            "Epoch 1800, Loss 0.2500\n",
            "Epoch 1900, Loss 0.2500\n",
            "Epoch 2000, Loss 0.2500\n",
            "Epoch 2100, Loss 0.2500\n",
            "Epoch 2200, Loss 0.2500\n",
            "Epoch 2300, Loss 0.2500\n",
            "Epoch 2400, Loss 0.2500\n",
            "Epoch 2500, Loss 0.2500\n",
            "Epoch 2600, Loss 0.2500\n",
            "Epoch 2700, Loss 0.2500\n",
            "Epoch 2800, Loss 0.2500\n",
            "Epoch 2900, Loss 0.2500\n",
            "Epoch 3000, Loss 0.2500\n",
            "Epoch 3100, Loss 0.2500\n",
            "Epoch 3200, Loss 0.2500\n",
            "Epoch 3300, Loss 0.2500\n",
            "Epoch 3400, Loss 0.2500\n",
            "Epoch 3500, Loss 0.2500\n",
            "Epoch 3600, Loss 0.2500\n",
            "Epoch 3700, Loss 0.2500\n",
            "Epoch 3800, Loss 0.2500\n",
            "Epoch 3900, Loss 0.2500\n",
            "Epoch 4000, Loss 0.2500\n",
            "Epoch 4100, Loss 0.2500\n",
            "Epoch 4200, Loss 0.2500\n",
            "Epoch 4300, Loss 0.2500\n",
            "Epoch 4400, Loss 0.2500\n",
            "Epoch 4500, Loss 0.2500\n",
            "Epoch 4600, Loss 0.2500\n",
            "Epoch 4700, Loss 0.2500\n",
            "Epoch 4800, Loss 0.2500\n",
            "Epoch 4900, Loss 0.2500\n",
            "Epoch 5000, Loss 0.2500\n",
            "Epoch 5100, Loss 0.2500\n",
            "Epoch 5200, Loss 0.2500\n",
            "Epoch 5300, Loss 0.2500\n",
            "Epoch 5400, Loss 0.2500\n",
            "Epoch 5500, Loss 0.2500\n",
            "Epoch 5600, Loss 0.2500\n",
            "Epoch 5700, Loss 0.2500\n",
            "Epoch 5800, Loss 0.2500\n",
            "Epoch 5900, Loss 0.2500\n",
            "Epoch 6000, Loss 0.2500\n",
            "Epoch 6100, Loss 0.2500\n",
            "Epoch 6200, Loss 0.2500\n",
            "Epoch 6300, Loss 0.2500\n",
            "Epoch 6400, Loss 0.2500\n",
            "Epoch 6500, Loss 0.2500\n",
            "Epoch 6600, Loss 0.2500\n",
            "Epoch 6700, Loss 0.2500\n",
            "Epoch 6800, Loss 0.2500\n",
            "Epoch 6900, Loss 0.2500\n",
            "Epoch 7000, Loss 0.2500\n",
            "Epoch 7100, Loss 0.2500\n",
            "Epoch 7200, Loss 0.2500\n",
            "Epoch 7300, Loss 0.2500\n",
            "Epoch 7400, Loss 0.2500\n",
            "Epoch 7500, Loss 0.2500\n",
            "Epoch 7600, Loss 0.2500\n",
            "Epoch 7700, Loss 0.2500\n",
            "Epoch 7800, Loss 0.2500\n",
            "Epoch 7900, Loss 0.2500\n",
            "Epoch 8000, Loss 0.2500\n",
            "Epoch 8100, Loss 0.2500\n",
            "Epoch 8200, Loss 0.2500\n",
            "Epoch 8300, Loss 0.2500\n",
            "Epoch 8400, Loss 0.2500\n",
            "Epoch 8500, Loss 0.2500\n",
            "Epoch 8600, Loss 0.2500\n",
            "Epoch 8700, Loss 0.2500\n",
            "Epoch 8800, Loss 0.2500\n",
            "Epoch 8900, Loss 0.2500\n",
            "Epoch 9000, Loss 0.2500\n",
            "Epoch 9100, Loss 0.2500\n",
            "Epoch 9200, Loss 0.2500\n",
            "Epoch 9300, Loss 0.2500\n",
            "Epoch 9400, Loss 0.2500\n",
            "Epoch 9500, Loss 0.2500\n",
            "Epoch 9600, Loss 0.2500\n",
            "Epoch 9700, Loss 0.2500\n",
            "Epoch 9800, Loss 0.2500\n",
            "Epoch 9900, Loss 0.2500\n",
            "\n",
            "Predictions:\n",
            "Input: [0. 0.], Target: 0.0, Prediction: 0.5000\n",
            "Input: [0. 1.], Target: 1.0, Prediction: 0.5000\n",
            "Input: [1. 0.], Target: 1.0, Prediction: 0.5000\n",
            "Input: [1. 1.], Target: 0.0, Prediction: 0.5000\n"
          ]
        }
      ],
      "source": [
        " import numpy as np\n",
        "\n",
        "class SimpleNeuralNetwork:\n",
        "      def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01):\n",
        "          self.learning_rate = learning_rate\n",
        "\n",
        "          self.W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
        "          self.b1 = np.zeros((1, hidden_size))\n",
        "          self.W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
        "          self.b2 = np.zeros((1, output_size))\n",
        "\n",
        "      def sigmoid(self, x):\n",
        "          return 1 / (1 + np.exp(-np.clip(x, -250, 250)))\n",
        "\n",
        "      def sigmoid_derivative(self, x):\n",
        "          return x * (1 - x)\n",
        "\n",
        "      def foward(self, X):\n",
        "          self.z1 = np.dot(X, self.W1) + self.b1\n",
        "          self.a1 = self.sigmoid(self.z1)\n",
        "          self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
        "          self.a2 = self.sigmoid(self.z2)\n",
        "          return self.a2\n",
        "\n",
        "      def backward(self, X, y, output):\n",
        "          m = X.shape[0]\n",
        "\n",
        "          dZ2 = output - y\n",
        "          dW2 = np.dot(self.a1.T, dZ2) / m\n",
        "          db2 = np.sum(dZ2, axis=0, keepdims=True) / m\n",
        "\n",
        "          dA1 = np.dot(dZ2, self.W2.T)\n",
        "          dZ1 = dA1 * self.sigmoid_derivative(self.a1)\n",
        "          dW1 = np.dot(X.T, dZ1) / m # Corrected dw1 to dW1 and added /m\n",
        "          db1 = np.sum(dZ1, axis=0, keepdims=True) / m # Added /m for consistency\n",
        "\n",
        "          return dW1, db1, dW2, db2 # Added return statement\n",
        "\n",
        "      def update_parameters(self, dW1, db1, dW2, db2): # Renamed to update_parameters\n",
        "          self.W1 -= self.learning_rate * dW1 # Corrected update rule\n",
        "          self.b1 -= self.learning_rate * db1 # Corrected update rule\n",
        "          self.W2 -= self.learning_rate * dW2 # Corrected update rule\n",
        "          self.b2 -= self.learning_rate * db2 # Corrected update rule\n",
        "\n",
        "      def train(self, X, y, epochs=1000):\n",
        "          for epoch in range(epochs):\n",
        "              output = self.foward(X)\n",
        "              loss = np.mean((output - y)**2)\n",
        "              dW1, db1, dW2, db2 = self.backward(X, y, output)\n",
        "              self.update_parameters(dW1, db1, dW2, db2)\n",
        "              if epoch % 100 == 0:\n",
        "                 print(f\"Epoch {epoch}, Loss {loss:.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "   X = np.array([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]])\n",
        "   y = np.array([[0.0], [1.0], [1.0], [0.0]])\n",
        "\n",
        "   nn = SimpleNeuralNetwork(input_size=2, hidden_size=4, output_size=1, learning_rate=0.1)\n",
        "   nn.train(X, y, epochs=10000)\n",
        "\n",
        "   prediction = nn.foward(X)\n",
        "   print(\"\\nPredictions:\") # Corrected /n to \\n\n",
        "   for i in range(len(X)): # Corrected x to X\n",
        "       print(f\"Input: {X[i]}, Target: {y[i][0]}, Prediction: {prediction[i][0]:.4f}\") # Corrected predictions to prediction"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOrIeY0cSxMBu+T7+EXd8LH",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}